## Интеграция Naive RAG (Этап 2) — описание и задачи для команды

Этот документ описывает:

- что уже реализовано в рамках моей части проекта — интеграция Naive RAG с оптимальными параметрами;
- какие модули/функции ожидаются от остальных членов команды, чтобы вся система заработала целиком.

---

## 1. Что уже сделано (моя часть)

### 1.1. Точка входа: `main.py`

- `main.py` теперь является основной точкой входа для всего Naive RAG пайплайна.
- Логика:
  - импортирует `load_config` из `rag_config.py`;
  - импортирует `init_rag_pipeline` и `run_cli_session` из `pipeline.py`;
  - в `main()`:
    - загружает конфиг (оптимальные параметры),
    - инициализирует RAG-пайплайн,
    - запускает CLI-сессию для интерактивных запросов.

**Главная идея**: запустив `python main.py`, мы попадаем в готовый Naive RAG с оптимальными параметрами.

---

### 1.2. Конфигурация: `rag_config.py`

- Создан датакласс `RagConfig` с основными параметрами Naive RAG:

  - `k`: количество документов, возвращаемых retriever’ом;
  - `model_name`: название модели LLM (например, `gpt-4.1-mini` или любая другая, которую используем);
  - `temperature`: температура генерации;
  - `max_tokens`: максимальное количество токенов в ответе.

- Функция `load_config()`:
  - возвращает экземпляр `RagConfig` с текущими значениями;
  - служит единым местом, куда можно подставить «оптимальные параметры», найденные в ходе экспериментов;
  - при желании легко заменяется на чтение `.yaml`/`.json` файла.

**Главная идея**: все оптимальные гиперпараметры Naive RAG задаются централизованно через `RagConfig` / `load_config()`.

---

### 1.3. Интеграционный модуль: `pipeline.py`

`pipeline.py` отвечает за склейку всех частей в один рабочий пайплайн и интерфейс.

Основные элементы:

- `_build_retriever(config: RagConfig)`:

  - ожидает, что будет модуль `vector_store.py` с функцией:

    ```python
    def build_vector_store():
        ...
        return vector_store
    ```

  - внутри:

    - вызывает `build_vector_store()` для получения векторного хранилища;
    - строит retriever стандартным способом LangChain:

      ```python
      retriever = vector_store.as_retriever(search_kwargs={"k": config.k})
      ```

- `_build_llm(config: RagConfig)`:

  - использует `langchain_openai.ChatOpenAI` (можно заменить на другой LLM-клиент при необходимости);
  - берёт из `config`:
    - `model_name`,
    - `temperature`,
    - `max_tokens`.

- `init_rag_pipeline(config: RagConfig)`:

  - получает `retriever` через `_build_retriever(config)`;
  - создаёт LLM через `_build_llm(config)`;
  - собирает Naive RAG через `langchain.chains.RetrievalQA.from_chain_type(...)` с `return_source_documents=True`.

  **Результат**: объект `rag_chain`, у которого можно вызвать `invoke({"query": ...})` и получить ответ + документы-источники.

- `run_cli_session(rag_chain, config)`:

  - печатает краткую информацию о текущих параметрах (модель, `k`);
  - запускает цикл:
    - читает вопрос пользователя из консоли;
    - отправляет его в `rag_chain`;
    - печатает текст ответа;
    - при наличии печатает список источников (по метаданным `source`/`title`).

**Главная идея**: `pipeline.py` — это интеграционный слой, который соединяет результаты остальных этапов (векторное хранилище, параметры, LLM) и даёт единый интерфейс использования.

---

## 2. Что нужно сделать остальным членам команды

### 2.1. Реализовать модуль `vector_store.py`

**Ответственные**: участники, которые занимались подготовкой данных, эмбеддингами и выбором векторного хранилища.

Необходимый интерфейс:

```python
# vector_store.py

def build_vector_store():
    """
    Создать или загрузить векторное хранилище на основе подготовленных данных.
    Должен вернуть объект, совместимый с LangChain VectorStore,
    у которого есть метод .as_retriever(search_kwargs={...}).
    """
    ...
    return vector_store
```

Требования:

- Векторное хранилище должно быть построено на основе наших документов (например, `kaztelecom.pdf` и других источников).
- Объект `vector_store` должен:
  - поддерживать метод `.as_retriever(search_kwargs={"k": ...})`;
  - корректно возвращать документы с метаданными (`metadata["source"]` или `metadata["title"]`), чтобы CLI мог показывать источники.

---

### 2.2. Зафиксировать оптимальные параметры Naive RAG

**Ответственные**: участники, которые проводили эксперименты по подбору:

- `k` (сколько документов брать в контекст),
- выбор модели LLM,
- `temperature`, `max_tokens`,
- возможно, других параметров (top_p, penalties и т.п., если нужно).

Варианты интеграции:

1. Простой вариант (минимум кода):

   - просто изменить значения по умолчанию в `RagConfig` в `rag_config.py`:
     - `k` установить в найденное оптимальное;
     - `model_name` — в реально используемую модель;
     - `temperature`, `max_tokens` — по результатам экспериментов.

2. Более формальный вариант:
   - реализовать чтение конфигурации из файла (например, `config/rag_naive_optimal.yaml`);
   - внутри `load_config()`:
     - считать файл,
     - подставить значения в `RagConfig`.

Важно:

- Эти параметры должны быть задокументированы (кратко в README/отчёте): какие значения выбраны и почему.

---

### 2.3. Установить и согласовать зависимости

**Ответственный**: любой, кто ведёт общий `requirements.txt` / `pyproject.toml`.

Необходимо:

- добавить библиотеки, которые реально используются:
  - LangChain (`langchain`);
  - клиент для LLM (например, `langchain-openai` или другой, в зависимости от выбранного поставщика);
  - библиотеку под векторное хранилище (например, `faiss-cpu`, `chromadb` и т.п.).

Также:

- определиться, как мы передаём API-ключи/конфиги для LLM (через `.env`, переменные окружения, конфиг-файл и т.п.).

---

## 3. Как запускать проект (после того, как всё будет готово)

1. Установить зависимости:

   ```bash
   pip install -r requirements.txt
   ```

2. Убедиться, что:

   - реализован `vector_store.py` с `build_vector_store()`;
   - в `rag_config.py` заданы актуальные оптимальные параметры;
   - доступны нужные переменные окружения (например, `OPENAI_API_KEY`, если используется OpenAI).

3. Запустить:

   ```bash
   python main.py
   ```

4. В консоли:

   - ввести вопрос по данным (например, по содержанию `kaztelecom.pdf`);
   - получить ответ модели и список источников, из которых этот ответ сформирован.

---

## 4. Итоговая картина по ролям

- Моя часть (уже сделано):

  - `main.py` как точка входа;
  - `rag_config.py` — конфиг и место для оптимальных параметров;
  - `pipeline.py` — сборка Naive RAG и CLI.

- Остальные участники:
  - подготовка данных и построение векторного индекса;
  - реализация `vector_store.build_vector_store()` с корректными метаданными;
  - фиксация и перенос оптимальных параметров в `rag_config.py` (или конфиг-файл);
  - оформление зависимостей и, если нужно, документации по запуску.
