{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "outputs": [],
   "execution_count": null,
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Step 2: Chunking Coherence Evaluation\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook implements the second step of the RAG pipeline project: evaluating different chunking strategies based on their semantic coherence.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 1. Install Dependencies\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"!pip install langchain langchain_experimental langchain_community sentence-transformers numpy matplotlib seaborn pandas scikit-learn nltk\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 2. Imports\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import os\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import nltk\\n\",\n",
    "    \"from nltk.tokenize import sent_tokenize\\n\",\n",
    "    \"from typing import List, Dict, Any\\n\",\n",
    "    \"\\n\",\n",
    "    \"from langchain.text_splitter import (\\n\",\n",
    "    \"    RecursiveCharacterTextSplitter,\\n\",\n",
    "    \"    CharacterTextSplitter\\n\",\n",
    "    \")\\n\",\n",
    "    \"from langchain_experimental.text_splitter import SemanticChunker\\n\",\n",
    "    \"from langchain_community.embeddings import HuggingFaceEmbeddings\\n\",\n",
    "    \"from sentence_transformers import SentenceTransformer\\n\",\n",
    "    \"from sklearn.metrics.pairwise import cosine_similarity\\n\",\n",
    "    \"\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Download sentence tokenizer model\\n\",\n",
    "    \"nltk.download('punkt')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 3. Load Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"Load the extracted text from the markdown files generated in Step 1.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def read_file(path):\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        with open(path, 'r', encoding='utf-8') as f:\\n\",\n",
    "    \"            return f.read()\\n\",\n",
    "    \"    except FileNotFoundError:\\n\",\n",
    "    \"        print(f\\\"Error: File not found at {path}\\\")\\n\",\n",
    "    \"        return \\\"\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Combine text from extracted pages\\n\",\n",
    "    \"text_files = [\\n\",\n",
    "    \"    'extraction_output/page_2_extracted.md',\\n\",\n",
    "    \"    'extraction_output/page_3_extracted.md'\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"full_text = \\\"\\\"\\n\",\n",
    "    \"for text_path in text_files:\\n\",\n",
    "    \"    content = read_file(text_path)\\n\",\n",
    "    \"    if content:\\n\",\n",
    "    \"        full_text += content + \\\"\\\\n\\\\n\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"if full_text.strip():\\n\",\n",
    "    \"    print(f\\\"Successfully loaded text. Total length: {len(full_text)} characters.\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"No text loaded. Using dummy text for demonstration.\\\")\\n\",\n",
    "    \"    full_text = \\\"This is a sample text. It is used to demonstrate the chunking strategies. \\\" * 50\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 4. Initialize Models\\n\",\n",
    "    \"\\n\",\n",
    "    \"We will use `all-MiniLM-L6-v2` for generating embeddings. It's a fast and effective model for semantic similarity.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Initialize SentenceTransformer for coherence calculation\\n\",\n",
    "    \"embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Initialize HuggingFaceEmbeddings for SemanticChunker\\n\",\n",
    "    \"hf_embeddings = HuggingFaceEmbeddings(model_name=\\\"all-MiniLM-L6-v2\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 5. Define Coherence Metric\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Metric: Semantic Coherence Score**\\n\",\n",
    "    \"\\n\",\n",
    "    \"The score is calculated as the average cosine similarity between the embeddings of sentences within a chunk and the mean embedding (centroid) of that chunk. A higher score indicates that the sentences in the chunk are semantically close to the central theme of the chunk.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def calculate_coherence_score(chunk_text: str) -> float:\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Calculates the semantic coherence score of a text chunk.\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    sentences = sent_tokenize(chunk_text)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # If chunk has 0 or 1 sentence, coherence is considered perfect (1.0) or undefined. \\n\",\n",
    "    \"    # We'll return 1.0 for single sentences as they are self-coherent.\\n\",\n",
    "    \"    if len(sentences) <= 1:\\n\",\n",
    "    \"        return 1.0\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Get embeddings for all sentences\\n\",\n",
    "    \"    embeddings = embedding_model.encode(sentences)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate the mean embedding (centroid) of the chunk\\n\",\n",
    "    \"    centroid = np.mean(embeddings, axis=0).reshape(1, -1)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate cosine similarity of each sentence to the centroid\\n\",\n",
    "    \"    similarities = cosine_similarity(embeddings, centroid)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Return the average similarity\\n\",\n",
    "    \"    return np.mean(similarities)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 6. Define Chunking Strategies\\n\",\n",
    "    \"\\n\",\n",
    "    \"We will implement the following strategies:\\n\",\n",
    "    \"1. **Fixed-size chunking** (200, 500, 1000 tokens)\\n\",\n",
    "    \"2. **Sentence-based chunking** (3, 5, 10 sentences)\\n\",\n",
    "    \"3. **Recursive chunking**\\n\",\n",
    "    \"4. **Semantic chunking**\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def get_fixed_size_chunks(text: str, chunk_size: int, chunk_overlap: int = 0) -> List[str]:\\n\",\n",
    "    \"    splitter = CharacterTextSplitter(\\n\",\n",
    "    \"        separator=\\\" \\\",\\n\",\n",
    "    \"        chunk_size=chunk_size,\\n\",\n",
    "    \"        chunk_overlap=chunk_overlap,\\n\",\n",
    "    \"        length_function=len\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    return splitter.split_text(text)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def get_sentence_chunks(text: str, n_sentences: int) -> List[str]:\\n\",\n",
    "    \"    sentences = sent_tokenize(text)\\n\",\n",
    "    \"    chunks = []\\n\",\n",
    "    \"    for i in range(0, len(sentences), n_sentences):\\n\",\n",
    "    \"        chunk = \\\" \\\".join(sentences[i:i + n_sentences])\\n\",\n",
    "    \"        chunks.append(chunk)\\n\",\n",
    "    \"    return chunks\\n\",\n",
    "    \"\\n\",\n",
    "    \"def get_recursive_chunks(text: str, chunk_size: int = 500, chunk_overlap: int = 50) -> List[str]:\\n\",\n",
    "    \"    splitter = RecursiveCharacterTextSplitter(\\n\",\n",
    "    \"        chunk_size=chunk_size,\\n\",\n",
    "    \"        chunk_overlap=chunk_overlap,\\n\",\n",
    "    \"        length_function=len,\\n\",\n",
    "    \"        separators=[\\\"\\\\n\\\\n\\\", \\\"\\\\n\\\", \\\" \\\", \\\"\\\"]\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    return splitter.split_text(text)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def get_semantic_chunks(text: str) -> List[str]:\\n\",\n",
    "    \"    splitter = SemanticChunker(hf_embeddings)\\n\",\n",
    "    \"    docs = splitter.create_documents([text])\\n\",\n",
    "    \"    return [doc.page_content for doc in docs]\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 7. Run Experiments\\n\",\n",
    "    \"\\n\",\n",
    "    \"We will iterate through all strategies, generate chunks, and calculate their coherence scores.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"results = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define experiments configuration\\n\",\n",
    "    \"experiments = [\\n\",\n",
    "    \"    (\\\"Fixed-200\\\", lambda t: get_fixed_size_chunks(t, 200)),\\n\",\n",
    "    \"    (\\\"Fixed-500\\\", lambda t: get_fixed_size_chunks(t, 500)),\\n\",\n",
    "    \"    (\\\"Fixed-1000\\\", lambda t: get_fixed_size_chunks(t, 1000)),\\n\",\n",
    "    \"    (\\\"Sentence-3\\\", lambda t: get_sentence_chunks(t, 3)),\\n\",\n",
    "    \"    (\\\"Sentence-5\\\", lambda t: get_sentence_chunks(t, 5)),\\n\",\n",
    "    \"    (\\\"Sentence-10\\\", lambda t: get_sentence_chunks(t, 10)),\\n\",\n",
    "    \"    (\\\"Recursive-500\\\", lambda t: get_recursive_chunks(t, 500)),\\n\",\n",
    "    \"    (\\\"Semantic\\\", lambda t: get_semantic_chunks(t))\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Running experiments...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"for name, func in experiments:\\n\",\n",
    "    \"    print(f\\\"Processing: {name}\\\")\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        chunks = func(full_text)\\n\",\n",
    "    \"        if not chunks:\\n\",\n",
    "    \"            print(f\\\"  Warning: No chunks generated for {name}\\\")\\n\",\n",
    "    \"            continue\\n\",\n",
    "    \"            \\n\",\n",
    "    \"        scores = [calculate_coherence_score(chunk) for chunk in chunks]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        avg_coherence = np.mean(scores)\\n\",\n",
    "    \"        variance = np.var(scores)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        results.append({\\n\",\n",
    "    \"            \\\"Chunking Method\\\": name,\\n\",\n",
    "    \"            \\\"Avg Coherence\\\": avg_coherence,\\n\",\n",
    "    \"            \\\"Avg Variance\\\": variance,\\n\",\n",
    "    \"            \\\"Num Chunks\\\": len(chunks),\\n\",\n",
    "    \"            \\\"Scores\\\": scores  # Store all scores for visualization\\n\",\n",
    "    \"        })\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"  Error in {name}: {e}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Experiments completed.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 8. Analysis and Visualization\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create DataFrame\\n\",\n",
    "    \"df_results = pd.DataFrame(results).drop(columns=[\\\"Scores\\\"])\\n\",\n",
    "    \"display(df_results)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visualization: Bar Chart of Avg Coherence\\n\",\n",
    "    \"plt.figure(figsize=(12, 6))\\n\",\n",
    "    \"sns.barplot(x=\\\"Chunking Method\\\", y=\\\"Avg Coherence\\\", data=df_results, palette=\\\"viridis\\\")\\n\",\n",
    "    \"plt.title(\\\"Average Semantic Coherence by Chunking Strategy\\\")\\n\",\n",
    "    \"plt.ylim(0, 1.0)\\n\",\n",
    "    \"plt.xticks(rotation=45)\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visualization: Box Plot of Score Distributions\\n\",\n",
    "    \"plt.figure(figsize=(12, 6))\\n\",\n",
    "    \"all_scores = []\\n\",\n",
    "    \"for res in results:\\n\",\n",
    "    \"    for score in res[\\\"Scores\\\"]:\\n\",\n",
    "    \"        all_scores.append({\\\"Method\\\": res[\\\"Chunking Method\\\"], \\\"Score\\\": score})\\n\",\n",
    "    \"\\n\",\n",
    "    \"df_scores = pd.DataFrame(all_scores)\\n\",\n",
    "    \"sns.boxplot(x=\\\"Method\\\", y=\\\"Score\\\", data=df_scores, palette=\\\"Set2\\\")\\n\",\n",
    "    \"plt.title(\\\"Distribution of Coherence Scores\\\")\\n\",\n",
    "    \"plt.xticks(rotation=45)\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 9. Recommendation\\n\",\n",
    "    \"\\n\",\n",
    "    \"Based on the results above, we can identify the strategy that provides the highest average coherence with reasonable variance.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"best_method = df_results.loc[df_results['Avg Coherence'].idxmax()]\\n\",\n",
    "    \"print(f\\\"Recommended Strategy: {best_method['Chunking Method']}\\\")\\n\",\n",
    "    \"print(f\\\"Reason: Highest average coherence score of {best_method['Avg Coherence']:.4f}\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.9.6\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 2\n",
    "}"
   ],
   "id": "4f4558c7359c282b"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
