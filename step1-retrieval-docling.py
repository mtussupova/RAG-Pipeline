"""
Step 1: Data Extraction & Validation
–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PDF —Å –ø–æ–º–æ—â—å—é Docling –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —á–µ—Ä–µ–∑ Visual LLM
"""

import json
import base64
from pathlib import Path

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env
from dotenv import load_dotenv
load_dotenv()

# PDF processing
from docling.document_converter import DocumentConverter
import fitz  # PyMuPDF –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü

# LLM –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º OpenAI —Å vision)
from openai import OpenAI


# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
PDF_PATH = "kaztelecom.pdf"
PAGES_TO_EXTRACT = [2, 3]  # –°—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (1-indexed)
OUTPUT_DIR = Path("extraction_output")


def create_page_screenshot(pdf_path: str, page_num: int, dpi: int = 150) -> bytes:
    """
    –°–æ–∑–¥–∞—ë—Ç —Å–∫—Ä–∏–Ω—à–æ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã PDF –≤ —Ñ–æ—Ä–º–∞—Ç–µ PNG.

    Args:
        pdf_path: –ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É
        page_num: –ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã (1-indexed)
        dpi: –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è

    Returns:
        PNG –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –±–∞–π—Ç–∞—Ö
    """
    doc = fitz.open(pdf_path)
    page = doc[page_num - 1]  # fitz –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 0-indexed

    # –°–æ–∑–¥–∞—ë–º –º–∞—Ç—Ä–∏—Ü—É –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –Ω—É–∂–Ω–æ–≥–æ DPI
    zoom = dpi / 72  # 72 - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π DPI –¥–ª—è PDF
    matrix = fitz.Matrix(zoom, zoom)

    # –†–µ–Ω–¥–µ—Ä–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    pix = page.get_pixmap(matrix=matrix)
    png_bytes = pix.tobytes("png")

    doc.close()
    return png_bytes


def extract_text_with_docling(pdf_path: str, pages: list[int]) -> dict[int, str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü PDF —Å –ø–æ–º–æ—â—å—é Docling.

    Args:
        pdf_path: –ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É
        pages: –°–ø–∏—Å–æ–∫ –Ω–æ–º–µ—Ä–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü (1-indexed)

    Returns:
        –°–ª–æ–≤–∞—Ä—å {–Ω–æ–º–µ—Ä_—Å—Ç—Ä–∞–Ω–∏—Ü—ã: –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π_—Ç–µ–∫—Å—Ç}
    """
    converter = DocumentConverter()
    result = converter.convert(pdf_path)
    doc = result.document

    # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π markdown
    full_markdown = doc.export_to_markdown()

    # Docling –Ω–µ –≤—Å–µ–≥–¥–∞ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º, –ø–æ—ç—Ç–æ–º—É –∏–∑–≤–ª–µ–∫–∞–µ–º –≤–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç
    # –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–≤ —Ä–µ–∞–ª—å–Ω–æ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ –Ω—É–∂–Ω–∞ –±–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –ª–æ–≥–∏–∫–∞)
    extracted = {}

    # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º —á–µ—Ä–µ–∑ –∏—Ç–µ—Ä–∞—Ü–∏—é –ø–æ —ç–ª–µ–º–µ–Ω—Ç–∞–º
    for page_num in pages:
        page_content = []
        for element, _level in doc.iterate_items():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ª–∏ —ç–ª–µ–º–µ–Ω—Ç –∫ –Ω—É–∂–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            if hasattr(element, 'prov') and element.prov:
                for prov in element.prov:
                    if hasattr(prov, 'page_no') and prov.page_no == page_num:
                        if hasattr(element, 'text') and element.text:
                            page_content.append(element.text)
                        elif hasattr(element, 'export_to_markdown'):
                            try:
                                page_content.append(element.export_to_markdown(doc))
                            except TypeError:
                                # Fallback –¥–ª—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –±–µ–∑ –º–µ—Ç–æ–¥–∞ –∏–ª–∏ —Å –¥—Ä—É–≥–æ–π —Å–∏–≥–Ω–∞—Ç—É—Ä–æ–π
                                pass
                        break

        if page_content:
            extracted[page_num] = "\n\n".join(page_content)
        else:
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–¥–µ–ª–∏—Ç—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç
            extracted[page_num] = full_markdown

    return extracted


def encode_image_to_base64(image_bytes: bytes) -> str:
    """–ö–æ–¥–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ base64."""
    return base64.b64encode(image_bytes).decode("utf-8")


def validate_extraction_with_llm(
    screenshot_base64: str,
    extracted_text: str,
    client: OpenAI
) -> dict:
    """
    –í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é Visual LLM.

    Args:
        screenshot_base64: –°–∫—Ä–∏–Ω—à–æ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ base64
        extracted_text: –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç/markdown
        client: OpenAI –∫–ª–∏–µ–Ω—Ç

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –æ—Ü–µ–Ω–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    """
    prompt = """–¢–µ–±–µ –¥–∞–Ω—ã:
1. –°–∫—Ä–∏–Ω—à–æ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã PDF
2. –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç/Markdown –∏–∑ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã

–û—Ü–µ–Ω–∏ –æ—Ç 1 –¥–æ 5 –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º:

**–°—Ç—Ä—É–∫—Ç—É—Ä–∞ (1-5):**
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (H1, H2, H3)?
- –ü—Ä–∞–≤–∏–ª—å–Ω–æ –ª–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω—ã —Å–ø–∏—Å–∫–∏ (–Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ, –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)?

**–¢–∞–±–ª–∏—Ü—ã (1-5):**
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–∞–±–ª–∏—Ü?
- –ß–∏—Ç–∞–µ–º—ã –ª–∏ –¥–∞–Ω–Ω—ã–µ –≤ —è—á–µ–π–∫–∞—Ö?
- –ü—Ä–∞–≤–∏–ª—å–Ω–æ –ª–∏ –≤—ã—Ä–æ–≤–Ω–µ–Ω—ã —Å—Ç–æ–ª–±—Ü—ã –∏ —Å—Ç—Ä–æ–∫–∏?

**–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (1-5):**
- –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –ª–∏ –∂–∏—Ä–Ω—ã–µ, –∫—É—Ä—Å–∏–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã?
- –ü—Ä–∞–≤–∏–ª—å–Ω–æ –ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã —Ñ–æ—Ä–º—É–ª—ã/—Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã?

**–ü–æ–ª–Ω–æ—Ç–∞ (1-5):**
- –í–µ—Å—å –ª–∏ —Ç–µ–∫—Å—Ç –∏–∑–≤–ª–µ—á—ë–Ω?
- –ù–µ—Ç –ª–∏ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤?

**–ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞:** (—Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)

–û—Ç–≤–µ—Ç –¥–∞–π –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:
{
  "structure_score": X,
  "tables_score": X,
  "formatting_score": X,
  "completeness_score": X,
  "overall_score": X,
  "comments": "..."
}"""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": prompt
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{screenshot_base64}"
                        }
                    },
                    {
                        "type": "text",
                        "text": f"–ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:\n\n{extracted_text}"
                    }
                ]
            }
        ],
        max_tokens=1000
    )

    # –ü–∞—Ä—Å–∏–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
    response_text = response.choices[0].message.content.strip()

    # –£–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ markdown –±–ª–æ–∫–∏
    if response_text.startswith("```"):
        response_text = response_text.split("```")[1]
        if response_text.startswith("json"):
            response_text = response_text[4:]

    return json.loads(response_text)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö."""

    # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    OUTPUT_DIR.mkdir(exist_ok=True)

    print("=" * 60)
    print("Step 1: Data Extraction & Validation")
    print("=" * 60)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º OpenAI –∫–ª–∏–µ–Ω—Ç
    client = OpenAI()

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é Docling
    print(f"\nüìÑ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ {PDF_PATH} (—Å—Ç—Ä–∞–Ω–∏—Ü—ã {PAGES_TO_EXTRACT})...")
    extracted_texts = extract_text_with_docling(PDF_PATH, PAGES_TO_EXTRACT)

    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º test cases
    test_cases = []

    for page_num in PAGES_TO_EXTRACT:
        print(f"\nüì∏ –°–æ–∑–¥–∞–Ω–∏–µ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}...")
        screenshot_bytes = create_page_screenshot(PDF_PATH, page_num)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç
        screenshot_path = OUTPUT_DIR / f"page_{page_num}.png"
        with open(screenshot_path, "wb") as f:
            f.write(screenshot_bytes)
        print(f"   –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {screenshot_path}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        text_path = OUTPUT_DIR / f"page_{page_num}_extracted.md"
        with open(text_path, "w", encoding="utf-8") as f:
            f.write(extracted_texts.get(page_num, ""))
        print(f"   –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {text_path}")

        test_cases.append({
            "page_num": page_num,
            "pdf_page_screenshot": str(screenshot_path),
            "extracted_text": extracted_texts.get(page_num, ""),
            "extraction_method": "docling"
        })

    # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é LLM
    print("\n" + "=" * 60)
    print("ü§ñ –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (LLM as a Judge)")
    print("=" * 60)

    results = []

    for case in test_cases:
        page_num = case["page_num"]
        print(f"\nüìä –û—Ü–µ–Ω–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}...")

        # –ß–∏—Ç–∞–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç –∏ –∫–æ–¥–∏—Ä—É–µ–º –≤ base64
        with open(case["pdf_page_screenshot"], "rb") as f:
            screenshot_base64 = encode_image_to_base64(f.read())

        # –ü–æ–ª—É—á–∞–µ–º –æ—Ü–µ–Ω–∫—É –æ—Ç LLM
        try:
            evaluation = validate_extraction_with_llm(
                screenshot_base64,
                case["extracted_text"],
                client
            )

            result = {
                "page_num": page_num,
                "extraction_method": case["extraction_method"],
                "evaluation": evaluation
            }
            results.append(result)

            # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            print(f"   –°—Ç—Ä—É–∫—Ç—É—Ä–∞:     {evaluation.get('structure_score', 'N/A')}/5")
            print(f"   –¢–∞–±–ª–∏—Ü—ã:       {evaluation.get('tables_score', 'N/A')}/5")
            print(f"   –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:{evaluation.get('formatting_score', 'N/A')}/5")
            print(f"   –ü–æ–ª–Ω–æ—Ç–∞:       {evaluation.get('completeness_score', 'N/A')}/5")
            print(f"   –ò—Ç–æ–≥–æ:         {evaluation.get('overall_score', 'N/A')}/5")
            print(f"   –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏:   {evaluation.get('comments', 'N/A')}")

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ: {e}")
            results.append({
                "page_num": page_num,
                "extraction_method": case["extraction_method"],
                "error": str(e)
            })

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results_path = OUTPUT_DIR / "validation_results.json"
    with open(results_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_path}")

    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "=" * 60)
    print("üìà –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
    print("=" * 60)

    successful_evals = [r for r in results if "evaluation" in r]
    if successful_evals:
        avg_overall = sum(r["evaluation"]["overall_score"] for r in successful_evals) / len(successful_evals)
        print(f"\n–°—Ä–µ–¥–Ω—è—è –∏—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞: {avg_overall:.2f}/5")

    print("\n‚úÖ –ì–æ—Ç–æ–≤–æ!")


if __name__ == "__main__":
    main()
